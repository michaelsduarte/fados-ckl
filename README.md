# fados-ckl
In this repository we have the codes used to simulation of the paper: Fully Adaptive Dictionary for Online Correntropy Kernel Learning Using Proximal Methods

# PART1
- Description: Comparison of the results of the OS-CKL algorithm with different criteria of dictionary construction
    OS-CKL (Online Sparse Correntropy Kernel Learning)
    ALD (Approximate Linear Dependence)
    NC (Novelty Criterion)
    CC (Coherence Criterion)
    SC (Surprise Criterion)
- Algorithm type: Recursive
- Dataset: Silverbox
- Prediction: One step ahead and Multi step ahead

# PART3
- Description: Comparison of the results of the OS-CKL algorithm with different criteria of dictionary construction
    OS-CKL (Online Sparse Correntropy Kernel Learning)
    ALD (Approximate Linear Dependence)
    NC (Novelty Criterion)
    CC (Coherence Criterion)
    SC (Surprise Criterion)
- Algorithm type: Recursive
- Dataset: Wiener-Hammerstein
- Prediction: One step ahead and Multi step ahead

# PART7
- Description: Comparison of the results of the OS-CKL algorithm with the novelty criterion and pruning
    OS-CKL (Online Sparse Correntropy Kernel Learning)
    NCL1 (Novelty Criterion with [or adaptive] L1-norm regularization function)
- Algorithm type: Recursive
- Dataset: Silverbox
- Prediction: One step ahead and Multi step ahead

# PART8
- Description: Comparison of the results of the OS-CKL algorithm with the novelty criterion and pruning
    OS-CKL (Online Sparse Correntropy Kernel Learning)
    NCL1 (Novelty Criterion with [or adaptive] L1-norm regularization function)
- Algorithm type: Recursive
- Dataset: Wiener-Hammerstein
- Prediction: One step ahead and Multi step ahead

# In addiotnal the codes of the algorithms:
- Kernel Recursive Maximum Correntropy using Novelty Criterion (KRMC-NC)
- Kernel Recursive Least Square using Approximate Linear Dependence (KRLS-ALD)
- Kernel Least Mean Square (KLMS)
- The codes proposed in this paper based on Correntropy Kernel Learning for differents sparsity inducing criteria

